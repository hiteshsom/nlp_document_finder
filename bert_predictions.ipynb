{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Q/A Model for document search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "    path = []\n",
    "    files = []\n",
    "    for dirpath, dirname, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            if not os.path.basename(dirpath).startswith('.'):\n",
    "                path.append(dirpath)\n",
    "                files.append(f)\n",
    "            \n",
    "    return path, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is attention mechanism\"\n",
    "true_answer = \"The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = os.getcwd()\n",
    "locations, documents = absoluteFilePaths(os.path.join(DIRECTORY, 'Google', 'research'))\n",
    "paths = [os.path.join(loc, doc) for loc, doc in zip(locations, documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/nlp.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/computer_vision.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/quantum_computing.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for path in paths:\n",
    "    if path.endswith('.pdf'):\n",
    "        contents.append(convert_pdf_to_txt(path))\n",
    "    else: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Large Cased SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/nlp.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/computer_vision.pdf',\n",
       " '/home/jupyter/nlp_document_finder/Google/research/quantum_computing.pdf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [re.sub(r'\\n', ' ', content) for content in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:27, 81.88s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for path, content in tqdm(zip(paths, contents)):\n",
    "    for i in range(0, len(content.split(\" \")), 50):\n",
    "        paragraph = content.split(\" \")[i:i+50]\n",
    "        encoding = tokenizer.encode_plus(text=question, text_pair=paragraph)\n",
    "        inputs = encoding['input_ids']  #Token embeddings\n",
    "        sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "        \n",
    "        start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores)\n",
    "        answer = ' '.join(tokens[start_index:end_index+1])\n",
    "        if start_index.numpy() < end_index.numpy():\n",
    "            answers.append([path, answer, i, i+start_index.numpy(), i+end_index.numpy(), (torch.max(start_scores)+torch.max(end_scores)).detach().numpy()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers = pd.DataFrame(data=answers, columns = ['path', 'answer', 'chunk', 'start_loc', 'end_loc', 'logit'])\n",
    "df_answers = df_answers.sort_values(by=['logit'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>a part of a neural architecture</td>\n",
       "      <td>1900</td>\n",
       "      <td>1909</td>\n",
       "      <td>1914</td>\n",
       "      <td>16.648014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>neural networks</td>\n",
       "      <td>14600</td>\n",
       "      <td>14612</td>\n",
       "      <td>14613</td>\n",
       "      <td>14.617985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>give emphasis to the input elements relevant to the task</td>\n",
       "      <td>3000</td>\n",
       "      <td>3036</td>\n",
       "      <td>3045</td>\n",
       "      <td>14.229467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>neural attention model</td>\n",
       "      <td>14250</td>\n",
       "      <td>14269</td>\n",
       "      <td>14271</td>\n",
       "      <td>13.741872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>to draw global [UNK] between input and [UNK]</td>\n",
       "      <td>650</td>\n",
       "      <td>677</td>\n",
       "      <td>684</td>\n",
       "      <td>13.261919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "2  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "3  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "4                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "\n",
       "                                                     answer  chunk  start_loc  \\\n",
       "0                           a part of a neural architecture   1900       1909   \n",
       "1                                           neural networks  14600      14612   \n",
       "2  give emphasis to the input elements relevant to the task   3000       3036   \n",
       "3                                    neural attention model  14250      14269   \n",
       "4              to draw global [UNK] between input and [UNK]    650        677   \n",
       "\n",
       "   end_loc      logit  \n",
       "0     1914  16.648014  \n",
       "1    14613  14.617985  \n",
       "2     3045  14.229467  \n",
       "3    14271  13.741872  \n",
       "4      684  13.261919  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation. The core idea behind attention\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "J. Zhao, “Inner attention based recurrent neural networks for answer selection,” in Proc. ACL, 2016, pp. 1288–1297. [69] C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou, “Attentive pooling  networks,” CoRR, vol. abs/1602.03609, 2016.  [70] Y. Cui, Z. Chen, S. Wei, S. Wang, T. Liu, and\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "documents.  More often than not, another input element q, called query,1 is used as a reference when computing the attention distribu- tion. In that case, the attention mechanism will give emphasis to the input elements relevant to the task according to q. If no query is deﬁned, attention will\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "pp. 1125–1135.  [55] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for abstractive sentence summarization,” in Proc. EMNLP, 2015, pp. 379–389.  [56] H. Li, J. Zhu, T. Liu, J. Zhang, and C. Zong, “Multi-modal sentence summarization with modality attention and image ﬁltering,” in Proc.\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/nlp.pdf\u001b[0m\n",
      " In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('\\033[1m' + df_answers.loc[i, 'path'] + '\\033[0m')\n",
    "    print(\" \".join(contents[paths.index(df_answers.loc[i, 'path'])].split(\" \")[df_answers.loc[i, 'chunk']: df_answers.loc[i, 'chunk']+50]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_answers.loc[i, 'f1'] = compute_f1(true_answer, df_answers.loc[i, 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>a part of a neural architecture</td>\n",
       "      <td>1900</td>\n",
       "      <td>1909</td>\n",
       "      <td>1914</td>\n",
       "      <td>16.648014</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>neural networks</td>\n",
       "      <td>14600</td>\n",
       "      <td>14612</td>\n",
       "      <td>14613</td>\n",
       "      <td>14.617985</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>give emphasis to the input elements relevant to the task</td>\n",
       "      <td>3000</td>\n",
       "      <td>3036</td>\n",
       "      <td>3045</td>\n",
       "      <td>14.229467</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>neural attention model</td>\n",
       "      <td>14250</td>\n",
       "      <td>14269</td>\n",
       "      <td>14271</td>\n",
       "      <td>13.741872</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>to draw global [UNK] between input and [UNK]</td>\n",
       "      <td>650</td>\n",
       "      <td>677</td>\n",
       "      <td>684</td>\n",
       "      <td>13.261919</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "2  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "3  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "4                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "\n",
       "                                                     answer  chunk  start_loc  \\\n",
       "0                           a part of a neural architecture   1900       1909   \n",
       "1                                           neural networks  14600      14612   \n",
       "2  give emphasis to the input elements relevant to the task   3000       3036   \n",
       "3                                    neural attention model  14250      14269   \n",
       "4              to draw global [UNK] between input and [UNK]    650        677   \n",
       "\n",
       "   end_loc      logit        f1  \n",
       "0     1914  16.648014  0.181818  \n",
       "1    14613  14.617985  0.047619  \n",
       "2     3045  14.229467  0.166667  \n",
       "3    14271  13.741872  0.093023  \n",
       "4      684  13.261919  0.083333  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Base Cased SQuAD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('deepset/bert-base-cased-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:45, 26.31s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for path, content in tqdm(zip(paths, contents)):\n",
    "    for i in range(0, len(content.split(\" \")), 50):\n",
    "        paragraph = content.split(\" \")[i:i+50]\n",
    "        encoding = tokenizer.encode_plus(text=question, text_pair=paragraph)\n",
    "        inputs = encoding['input_ids']  #Token embeddings\n",
    "        sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "        \n",
    "        start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores)\n",
    "        answer = ' '.join(tokens[start_index:end_index+1])\n",
    "        if start_index.numpy() < end_index.numpy():\n",
    "            answers.append([path, answer, i, i+start_index.numpy(), i+end_index.numpy(), (torch.max(start_scores)+torch.max(end_scores)).detach().numpy()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers = pd.DataFrame(data=answers, columns = ['path', 'answer', 'chunk', 'start_loc', 'end_loc', 'logit'])\n",
    "df_answers = df_answers.sort_values(by=['logit'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>generally embedded in larger neural network</td>\n",
       "      <td>1450</td>\n",
       "      <td>1471</td>\n",
       "      <td>1476</td>\n",
       "      <td>10.386783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>an instrument to [UNK] the input into a compact [UNK]</td>\n",
       "      <td>3600</td>\n",
       "      <td>3637</td>\n",
       "      <td>3646</td>\n",
       "      <td>9.970831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>[UNK] attention and [UNK] [UNK] the keys and the [UNK] are fed into a single neural [UNK]</td>\n",
       "      <td>6250</td>\n",
       "      <td>6259</td>\n",
       "      <td>6275</td>\n",
       "      <td>9.1312275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>to draw global [UNK] between input and [UNK]</td>\n",
       "      <td>650</td>\n",
       "      <td>677</td>\n",
       "      <td>684</td>\n",
       "      <td>8.929373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>different heads can capture local and global contexts at the same time</td>\n",
       "      <td>7950</td>\n",
       "      <td>7981</td>\n",
       "      <td>7992</td>\n",
       "      <td>8.845034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "2  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "3                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "4  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "\n",
       "                                                                                      answer  \\\n",
       "0                                                generally embedded in larger neural network   \n",
       "1                                      an instrument to [UNK] the input into a compact [UNK]   \n",
       "2  [UNK] attention and [UNK] [UNK] the keys and the [UNK] are fed into a single neural [UNK]   \n",
       "3                                               to draw global [UNK] between input and [UNK]   \n",
       "4                     different heads can capture local and global contexts at the same time   \n",
       "\n",
       "   chunk  start_loc  end_loc      logit  \n",
       "0   1450       1471     1476  10.386783  \n",
       "1   3600       3637     3646   9.970831  \n",
       "2   6250       6259     6275  9.1312275  \n",
       "3    650        677      684   8.929373  \n",
       "4   7950       7981     7992   8.845034  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "not offer a quantitative evaluation of different types of attention mechanisms since such mechanisms are generally embedded in larger neural network architectures devised to address  recommendation [22], [23], time-series analysis [24], [25], games [26], and mathematical problems [27], [28].  In NLP, after an initial exploration by a number\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "they are relevant in Table III.  For tasks such as document classiﬁcation, where usually there is only K in input and no query, the attention mechanism can be seen as an instrument to encode the input into a compact form. The computation of such an embedding can be seen\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "different size. In additive attention and concat attention, the keys and the queries are fed into a single neural layer. We speak instead of deep attention if multiple layers are  3Part of our terminology. As previously noted, wimp is termed context  vector by Yang et al. [52] and\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/nlp.pdf\u001b[0m\n",
      " In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "positions (the sets of weights), or on the outputs (the context vectors). Multihead attention can be especially helpful when combined with nonsoft attention distribution since different heads can capture local and global contexts at the same time [39]. Finally, labelwise attention [126] computes a separate attention distribution for each class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('\\033[1m' + df_answers.loc[i, 'path'] + '\\033[0m')\n",
    "    print(\" \".join(contents[paths.index(df_answers.loc[i, 'path'])].split(\" \")[df_answers.loc[i, 'chunk']: df_answers.loc[i, 'chunk']+50]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_answers.loc[i, 'f1'] = compute_f1(true_answer, df_answers.loc[i, 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>generally embedded in larger neural network</td>\n",
       "      <td>1450</td>\n",
       "      <td>1471</td>\n",
       "      <td>1476</td>\n",
       "      <td>10.386783</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>an instrument to [UNK] the input into a compact [UNK]</td>\n",
       "      <td>3600</td>\n",
       "      <td>3637</td>\n",
       "      <td>3646</td>\n",
       "      <td>9.970831</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>[UNK] attention and [UNK] [UNK] the keys and the [UNK] are fed into a single neural [UNK]</td>\n",
       "      <td>6250</td>\n",
       "      <td>6259</td>\n",
       "      <td>6275</td>\n",
       "      <td>9.1312275</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>to draw global [UNK] between input and [UNK]</td>\n",
       "      <td>650</td>\n",
       "      <td>677</td>\n",
       "      <td>684</td>\n",
       "      <td>8.929373</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>different heads can capture local and global contexts at the same time</td>\n",
       "      <td>7950</td>\n",
       "      <td>7981</td>\n",
       "      <td>7992</td>\n",
       "      <td>8.845034</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "2  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "3                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "4  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "\n",
       "                                                                                      answer  \\\n",
       "0                                                generally embedded in larger neural network   \n",
       "1                                      an instrument to [UNK] the input into a compact [UNK]   \n",
       "2  [UNK] attention and [UNK] [UNK] the keys and the [UNK] are fed into a single neural [UNK]   \n",
       "3                                               to draw global [UNK] between input and [UNK]   \n",
       "4                     different heads can capture local and global contexts at the same time   \n",
       "\n",
       "   chunk  start_loc  end_loc      logit        f1  \n",
       "0   1450       1471     1476  10.386783  0.086957  \n",
       "1   3600       3637     3646   9.970831  0.085106  \n",
       "2   6250       6259     6275  9.1312275  0.074074  \n",
       "3    650        677      684   8.929373  0.083333  \n",
       "4   7950       7981     7992   8.845034  0.039216  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Large uncased SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:29, 82.48s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for path, content in tqdm(zip(paths, contents)):\n",
    "    for i in range(0, len(content.split(\" \")), 50):\n",
    "        paragraph = content.split(\" \")[i:i+50]\n",
    "        encoding = tokenizer.encode_plus(text=question, text_pair=paragraph)\n",
    "        inputs = encoding['input_ids']  #Token embeddings\n",
    "        sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "        \n",
    "        start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores)\n",
    "        answer = ' '.join(tokens[start_index:end_index+1])\n",
    "        if start_index.numpy() < end_index.numpy():\n",
    "            answers.append([path, answer, i, i+start_index.numpy(), i+end_index.numpy(), (torch.max(start_scores)+torch.max(end_scores)).detach().numpy()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers = pd.DataFrame(data=answers, columns = ['path', 'answer', 'chunk', 'start_loc', 'end_loc', 'logit'])\n",
    "df_answers = df_answers.sort_values(by=['logit'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>soft attention</td>\n",
       "      <td>7450</td>\n",
       "      <td>7491</td>\n",
       "      <td>7492</td>\n",
       "      <td>11.921171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>additive attention</td>\n",
       "      <td>1600</td>\n",
       "      <td>1623</td>\n",
       "      <td>1624</td>\n",
       "      <td>11.83612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>relating different positions</td>\n",
       "      <td>800</td>\n",
       "      <td>853</td>\n",
       "      <td>855</td>\n",
       "      <td>11.543196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>highlights the most relevant micro elements within each macro element</td>\n",
       "      <td>5750</td>\n",
       "      <td>5779</td>\n",
       "      <td>5788</td>\n",
       "      <td>11.416506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>attention model for natural language</td>\n",
       "      <td>15200</td>\n",
       "      <td>15231</td>\n",
       "      <td>15235</td>\n",
       "      <td>11.345589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "2                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "3  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "4  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "\n",
       "                                                                  answer  \\\n",
       "0                                                         soft attention   \n",
       "1                                                     additive attention   \n",
       "2                                           relating different positions   \n",
       "3  highlights the most relevant micro elements within each macro element   \n",
       "4                                   attention model for natural language   \n",
       "\n",
       "   chunk  start_loc  end_loc      logit  \n",
       "0   7450       7491     7492  11.921171  \n",
       "1   1600       1623     1624   11.83612  \n",
       "2    800        853      855  11.543196  \n",
       "3   5750       5779     5788  11.416506  \n",
       "4  15200      15231    15235  11.345589  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.  10  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  through soft attention applied to the same set of keys. The proper “softness” of the distribution\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/nlp.pdf\u001b[0m\n",
      "matrix multiplication code.  dk  While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/nlp.pdf\u001b[0m\n",
      "[12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "to the sequence of macroelement embeddings, in order to compute an embed- ding for the whole document D. With this model, attention ﬁrst highlights the most relevant micro elements within each macro element and, then, the most relevant macro elements in the document. For instance, Yang et al. [52] applied\n",
      "\n",
      "\u001b[1m/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf\u001b[0m\n",
      "entity description co-attention for entity disambiguation,” in Proc. AAAI, 2018, pp. 5908–5915.  [89] A. Parikh, O. Täckström, D. Das, and J. Uszkoreit, “A decomposable attention model for natural language inference,” in Proc. EMNLP, 2016, pp. 2249–2255.  [90] Y. Tay, A. T. Luu, and S. C. Hui, “Compare, compress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('\\033[1m' + df_answers.loc[i, 'path'] + '\\033[0m')\n",
    "    print(\" \".join(contents[paths.index(df_answers.loc[i, 'path'])].split(\" \")[df_answers.loc[i, 'chunk']: df_answers.loc[i, 'chunk']+50]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_answers.loc[i, 'f1'] = compute_f1(true_answer, df_answers.loc[i, 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>answer</th>\n",
       "      <th>chunk</th>\n",
       "      <th>start_loc</th>\n",
       "      <th>end_loc</th>\n",
       "      <th>logit</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>soft attention</td>\n",
       "      <td>7450</td>\n",
       "      <td>7491</td>\n",
       "      <td>7492</td>\n",
       "      <td>11.921171</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>additive attention</td>\n",
       "      <td>1600</td>\n",
       "      <td>1623</td>\n",
       "      <td>1624</td>\n",
       "      <td>11.83612</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/nlp.pdf</td>\n",
       "      <td>relating different positions</td>\n",
       "      <td>800</td>\n",
       "      <td>853</td>\n",
       "      <td>855</td>\n",
       "      <td>11.543196</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>highlights the most relevant micro elements within each macro element</td>\n",
       "      <td>5750</td>\n",
       "      <td>5779</td>\n",
       "      <td>5788</td>\n",
       "      <td>11.416506</td>\n",
       "      <td>0.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf</td>\n",
       "      <td>attention model for natural language</td>\n",
       "      <td>15200</td>\n",
       "      <td>15231</td>\n",
       "      <td>15235</td>\n",
       "      <td>11.345589</td>\n",
       "      <td>0.044444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             path  \\\n",
       "0  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "1                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "2                                       /home/jupyter/nlp_document_finder/Google/research/nlp.pdf   \n",
       "3  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "4  /home/jupyter/nlp_document_finder/Google/research/Attention in Natural Language Processing.pdf   \n",
       "\n",
       "                                                                  answer  \\\n",
       "0                                                         soft attention   \n",
       "1                                                     additive attention   \n",
       "2                                           relating different positions   \n",
       "3  highlights the most relevant micro elements within each macro element   \n",
       "4                                   attention model for natural language   \n",
       "\n",
       "   chunk  start_loc  end_loc      logit        f1  \n",
       "0   7450       7491     7492  11.921171  0.047619  \n",
       "1   1600       1623     1624   11.83612  0.047619  \n",
       "2    800        853      855  11.543196  0.000000  \n",
       "3   5750       5779     5788  11.416506  0.081633  \n",
       "4  15200      15231    15235  11.345589  0.044444  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
