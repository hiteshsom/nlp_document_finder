{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"D:/luke_squad_wikipedia_data/enwiki_20160305.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2squad(df, squad_version=\"v1.1\", output_dir=None, filename=None):\n",
    "    \"\"\"\n",
    "     Converts a pandas dataframe with columns ['title', 'paragraphs'] to a json file with SQuAD format.\n",
    "     Parameters\n",
    "    ----------\n",
    "     df : pandas.DataFrame\n",
    "         a pandas dataframe with columns ['title', 'paragraphs', 'question']\n",
    "     squad_version : str, optional\n",
    "         the SQuAD dataset version format (the default is 'v2.0')\n",
    "     output_dir : str, optional\n",
    "         Enable export of output (the default is None)\n",
    "     filename : str, optional\n",
    "         [description]\n",
    "    Returns\n",
    "    -------\n",
    "    json_data: dict\n",
    "        A json object with SQuAD format\n",
    "     Examples\n",
    "     --------\n",
    "     >>> from ast import literal_eval\n",
    "     >>> import pandas as pd\n",
    "     >>> from cdqa.utils.converters import df2squad\n",
    "     >>> from cdqa.utils.filters import filter_paragraphs\n",
    "     >>> df = pd.read_csv('../data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv', converters={'paragraphs': literal_eval})\n",
    "     >>> df['paragraphs'] = df['paragraphs'].apply(filter_paragraphs)\n",
    "     >>> json_data = df2squad(df=df, squad_version='v1.1', output_dir='../data', filename='bnpp_newsroom-v1.1')\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = {}\n",
    "    json_data[\"version\"] = squad_version\n",
    "    json_data[\"data\"] = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        temp = {\"title\": row[\"title\"], \"paragraphs\": []}\n",
    "        for paragraph in row[\"paragraphs\"]:\n",
    "            temp[\"paragraphs\"].append({\"context\": paragraph, \"qas\": [{'question': question}]})\n",
    "        json_data[\"data\"].append(temp)\n",
    "\n",
    "    if output_dir:\n",
    "        with open(os.path.join(output_dir, \"{}.json\".format(filename)), \"w\") as outfile:\n",
    "            json.dump(json_data, outfile)\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "    path = []\n",
    "    files = []\n",
    "    for dirpath, dirname, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            if not os.path.basename(dirpath).startswith('.'):\n",
    "                path.append(dirpath)\n",
    "                files.append(f)\n",
    "            \n",
    "    return path, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many people work at Amazon\"\n",
    "true_answer = \"Amazon directly employs 840,000workers worldwide\"\n",
    "\n",
    "# question = \"How many position were opened in March\"\n",
    "# true_answer = \"100000\"\n",
    "\n",
    "# question = \"How many new people hired by amazon\"\n",
    "# true_answer = \"100000\"\n",
    "\n",
    "# question = \"What are dominant sequence transduction models based on\"\n",
    "# true_answer = \" complex recurrent or convolutional neural networks that include an encoder and a decoder\"\n",
    "\n",
    "# question = \"What is attention mechanism\"\n",
    "# true_answer = \"The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation.\"\n",
    "\n",
    "# question = \"What is quantum entanglement\"\n",
    "# true_answer = \"Quantum Entanglement allows qubits that are separated by incredible distances to interact with each other instantaneously (not limited to the speed of light).\"\n",
    "\n",
    "# question = \"What are the applications of Face Swapping\"\n",
    "# true_answer = \"Face swapping has a number of compelling applications in video compositing, transfiguration in portraits, and especially in  identity  protection  as  it  can  replace  faces  in  photographs by ones from a collection of stock images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = os.getcwd()\n",
    "locations, documents = absoluteFilePaths(os.path.join(DIRECTORY, 'Google', 'research'))\n",
    "paths = [os.path.join(loc, doc) for loc, doc in zip(locations, documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\hiteshsom\\\\Documents\\\\nlp_document_finder\\\\Google\\\\research\\\\2019-Annual-Report-pages-1-5.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for path in paths:\n",
    "    if path.endswith('.pdf'):\n",
    "        contents.append(convert_pdf_to_txt(path))\n",
    "    else: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Large Cased SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\hiteshsom\\\\Documents\\\\nlp_document_finder\\\\Google\\\\research\\\\2019-Annual-Report-pages-1-5.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [re.sub(r'\\n', ' ', content) for content in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'paragraphs'])\n",
    "df.loc[0, 'title'] = '2019 Annual Report'\n",
    "df.loc[0, 'paragraphs'] = contents\n",
    "df.loc[0, 'question'] = question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 Annual Report</td>\n",
       "      <td>[2  0  1  9  A N N U A L  R E P O R T  \f",
       "To our shareowners:  One thing we’ve learned from the CO...</td>\n",
       "      <td>How many people work at Amazon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title  \\\n",
       "0  2019 Annual Report   \n",
       "\n",
       "                                                                                            paragraphs  \\\n",
       "0  [2  0  1  9  A N N U A L  R E P O R T  \n",
       "To our shareowners:  One thing we’ve learned from the CO...   \n",
       "\n",
       "                         question  \n",
       "0  How many people work at Amazon  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "json_data = df2squad(df, squad_version=\"v1.1\", output_dir='../', filename='luke_test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(json_data, open(\"D:/luke/data/test_data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiteshsom\\Documents\\env\\Scripts\\python.exe: No module named examples.cli\n"
     ]
    }
   ],
   "source": [
    "!cd C:\\Users\\hiteshsom\\Documents\\nlp_document_finder\\luke \n",
    "!python -m examples.cli \\\n",
    "    --model-file=luke_large_500k.tar.gz \\\n",
    "    --output-dir=D:\\luke\\output \\\n",
    "    reading-comprehension run \\\n",
    "    --data-dir=D:\\luke\\data \\\n",
    "    --checkpoint-file=D:/luke/checkpoint/pytorch_model.bin \\\n",
    "    --no-negative \\\n",
    "    --wiki-link-db-file=enwiki_20160305.pkl \\\n",
    "    --model-redirects-file=enwiki_20181220_redirects.pkl \\\n",
    "    --link-redirects-file=enwiki_20160305_redirects.pkl \\\n",
    "    --no-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia2vec\n",
      "  Downloading wikipedia2vec-1.0.4.tar.gz (1.2 MB)\n",
      "Requirement already satisfied: click in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (7.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hiteshsom\\documents\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (0.17.0)\n",
      "Requirement already satisfied: marisa-trie in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (0.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (1.5.4)\n",
      "Requirement already satisfied: six in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (1.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hiteshsom\\documents\\env\\lib\\site-packages (from wikipedia2vec) (4.56.0)\n",
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "Collecting lmdb\n",
      "  Downloading lmdb-1.1.1-cp38-cp38-win_amd64.whl (105 kB)\n",
      "Collecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6-cp38-cp38-win_amd64.whl (102 kB)\n",
      "Building wheels for collected packages: wikipedia2vec, jieba\n",
      "  Building wheel for wikipedia2vec (setup.py): started\n",
      "  Building wheel for wikipedia2vec (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia2vec: filename=wikipedia2vec-1.0.4-cp38-cp38-win_amd64.whl size=2068472 sha256=f46713e00e00ca39bc6fc31be7721817d3cf3b4c26478cc04ba920b8ac133ba2\n",
      "  Stored in directory: c:\\users\\hiteshsom\\appdata\\local\\pip\\cache\\wheels\\66\\7b\\2f\\33bdb0025161200c730444c4fadc6c8caf2d55bf47ccbe2720\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=302c3680e013499d4f75ffe6cf56fbb1206c7bcaa1183a023f3dfb58d8cd54af\n",
      "  Stored in directory: c:\\users\\hiteshsom\\appdata\\local\\pip\\cache\\wheels\\ca\\38\\d8\\dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built wikipedia2vec jieba\n",
      "Installing collected packages: mwparserfromhell, lmdb, jieba, wikipedia2vec\n",
      "Successfully installed jieba-0.42.1 lmdb-1.1.1 mwparserfromhell-0.6 wikipedia2vec-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
